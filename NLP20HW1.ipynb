{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2020 - HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Gathering and Cleaning Up Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ptb_preprocess will assist us in cleaning up the data according to the given rules. First, let's introduce some functions that will help us edit the tokens list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all numbers by the token N\n",
    "def replaceNums(tokens):\n",
    "    return [\"N\" if nltk.tokenize.punkt.PunktToken(w).is_number else w for w in tokens]\n",
    "\n",
    "# Keep only the top-10K most frequent words in the dataset\n",
    "def keep_only_top_freq(tokens, most_freq):\n",
    "    return [w if (w in most_freq) or (w == \"N\") else \"<unk>\" for w in tokens]\n",
    "\n",
    "def remove_punc_helper(w):\n",
    "    punctuations = list(string.punctuation)\n",
    "    return \"\".join([l for l in w if l not in punctuations])\n",
    "\n",
    "# Remove all punctuations\n",
    "def remove_punc(tokens):\n",
    "    return [remove_punc_helper(w) for w in tokens if not remove_punc_helper(w) == \"\"]\n",
    "\n",
    "# change to lowercase\n",
    "def lower_case(tokens):\n",
    "    return [w.lower() for w in tokens]\n",
    "\n",
    "# Return an array of the top most frequent words in the dataset\n",
    "def get_top_most_freq(tokens,top):\n",
    "    fdistw = nltk.FreqDist(remove_punc(tokens))\n",
    "    return [w for (w, n) in fdistw.most_common(top)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ptb_preprocess will creata a two-dimensional array of tokenize words for every sentence, than it will use the functions defined above to edit the text and create the output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import nltk.data\n",
    "\n",
    "def ptb_preprocess(filenames, top=10000):\n",
    "    for fname in filenames:\n",
    "        f = open(fname, \"r\")\n",
    "        raw = f.read()\n",
    "\n",
    "        # create an array of the top most freqent words in the text, will be using it later:\n",
    "        most_freq = get_top_most_freq(word_tokenize(raw), top)\n",
    "\n",
    "        tokens = [word_tokenize(t) for t in sent_tokenize(raw)]\n",
    "\n",
    "        new_tokens = [(keep_only_top_freq(remove_punc(replaceNums(lower_case(s))), most_freq))\n",
    "                      for s in tokens]\n",
    "\n",
    "\n",
    "        new_tokens = [\" \".join(t) for t in new_tokens]\n",
    "\n",
    "        output = open(fname+ '.out', 'w+')\n",
    "\n",
    "        [output.write(sent+\"\\n\") for sent in new_tokens]\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function on some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green green green blue blue <unk> <unk> N\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('a.txt', 'w+')\n",
    "f.write(\"Green green green blue blue red orange 100\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "filenames = [\"a.txt\"]\n",
    "ptb_preprocess(filenames,2)\n",
    "\n",
    "f = open(\"a.txt.out\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the output is in lowercase, only the 2-most-frequent words 'blue' and 'green' remained, and the number \"100\" was replaced by the token 'N'. Let's look at another example where we can observe how words like \"don't\" and \"$12\" are splits into two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do nt caller s N\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('b.txt', 'w+')\n",
    "f.write(\"don't caller's $12\")\n",
    "f.close()\n",
    "\n",
    "filenames = [\"b.txt\"]\n",
    "ptb_preprocess(filenames)\n",
    "\n",
    "f = open(\"b.txt.out\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the function on 'Shakespeare works' and print the first 5 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen before we proceed any further hear me speak\n",
      "\n",
      "all speak speak\n",
      "\n",
      "first citizen you are all resolved rather to die than to famish\n",
      "\n",
      "all resolved\n",
      "\n",
      "resolved\n",
      "\n",
      "first citizen first you know <unk> <unk> is chief enemy to the people\n",
      "\n",
      "all we knowt we knowt\n",
      "\n",
      "first citizen let us kill him and we ll have corn at our own price\n",
      "\n",
      "ist a verdict\n",
      "\n",
      "all no more talking o nt let it be done away away\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "url = \"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\"\n",
    "urllib.request.urlretrieve(url, \"shakespeare_input.txt\")\n",
    "\n",
    "filenames = [\"shakespeare_input.txt\"]\n",
    "ptb_preprocess(filenames)\n",
    "\n",
    "with open(\"shakespeare_input.txt.out\") as myfile:\n",
    "    [print(next(myfile)) for x in range(10)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of what is a \"word\" thats adopted is Semantic. The tokens are split into independent units of meaning as we can obseve from the example word 'don't' which is seperted into 'do' and 'nt'. \n",
    "If we use a character level language model the definition of a word changes into an orthographic definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Gathering Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "\n",
    "def count_tokens(dataset):\n",
    "    return len(word_tokenize(dataset))\n",
    "\n",
    "def count_characters(dataset):\n",
    "    return len(dataset)\n",
    "\n",
    "def count_distinct_words(dataset):\n",
    "    return len(set((word_tokenize(dataset))))\n",
    "\n",
    "def count_most_N_frequent(dataset):\n",
    "    \n",
    "    most_frequent = (word for word in dataset.split() if not word == \"<unk>\")\n",
    "    return len(set(most_frequent))\n",
    "\n",
    "def token_type_ratio(dataset):\n",
    "    return count_tokens(dataset) / count_distinct_words(dataset)\n",
    "\n",
    "def dev_vs_train(dev_dataset, train_dataset):\n",
    "    set1 = set(dev_dataset)\n",
    "    set2 = set(train_dataset)\n",
    "\n",
    "    return len(set1.difference(set2))\n",
    "\n",
    "def avg_and_standart_deviation(dataset, characters_dataset):\n",
    "    mean = count_tokens(characters_dataset) / count_tokens(dataset)\n",
    "\n",
    "    sum = 0\n",
    "    for token in dataset.split():\n",
    "        sum += pow((len(token) - mean), 2)\n",
    "    standart_deviation = math.sqrt((sum / count_tokens(dataset)))\n",
    "\n",
    "    return (mean, standart_deviation)\n",
    "\n",
    "def diff_ngrams(dataset, n):\n",
    "\n",
    "    n_grams = list(ngrams(word_tokenize(dataset), n))\n",
    "    return len(n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will discuss our expectations in comparison to the results. We will eaxmine the statistics of the ptb train dataset.\n",
    "\n",
    "Our expectaion for number of tokens is around 800,000. The dataset is 400,000 lines long and contains about 20 words in each line.\n",
    "\n",
    "Our expectaion for number of characters is around: the number of tokens * 5 = 800,000 * 5 = 4,000,000 since we expect the words to be not too short and not too long.\n",
    "\n",
    "Our expectaion for number of distinct words is around 5000. The document is in English which contains 171,476 words (according to the English dictionary). Apparently, not all English words appear in the text, but a 5 percent is a likely estimate.\n",
    "\n",
    "Our expectaion for number of tokens corresponding to the top-N most frequent words in the vocabulary is 10000 because we calculate the top 10000 frequent words.\n",
    "\n",
    "Our expectaion for number of token/type is around 200 becuase we believe that there are a lot of words that repeat theirselves.\n",
    "\n",
    "Our expectaion for number of types that appear in the dev data but not the training data is around 100 since the dev data is much shorter than the training data.\n",
    "\n",
    "Our expectaion for average number and standard deviation of characters per token is around 5 characters per token with standart deviation of 1.\n",
    "\n",
    "Our expectation for the total number of distinct n-grams (of words) that appear in the dataset for n=2,3,4 is around 800,000 - n + 1 for each n. Because in a text of |T| words this is the number of possible n-grams.\n",
    "\n",
    "Our expectation for the total number of distinct n-grams (of characters) that appear in the dataset for n=2,3,4,5,6,7 is around 800,000 * 5 - n + 1 for each n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tokens is: 978726\n",
      "The total number of characters is: 5101618\n",
      "The total number of distinct words is: 9996\n",
      "The total number of tokens corresponding to the top-N most frequent words in the vocabulary is: 9998\n",
      "The token/type ratio in the dataset is: 97.91176470588235\n",
      "The number of types that appear in the dev data but not the training data is: 0\n",
      "The average number and standard deviation of characters per token are:  avg: 5.08356169142334 deviation: 2.5113567920948805\n",
      "The total number of distinct n-grams (of words) that appear in the dataset for n = 2 978725\n",
      "The total number of distinct n-grams (of words) that appear in the dataset for n = 3 978724\n",
      "The total number of distinct n-grams (of words) that appear in the dataset for n = 4 978723\n",
      "The total number of distint n-grams of characters that appear for n = 2 4975413\n",
      "The total number of distint n-grams of characters that appear for n = 3 4975412\n"
     ]
    }
   ],
   "source": [
    "text = open(\"ptb.train.txt\").read()\n",
    "characters_dataset = open(\"ptb.char.train.txt\").read()\n",
    "dev_dataset = open(\"ptb.valid.txt\").read()\n",
    "\n",
    "\n",
    "tokens = count_tokens(text)\n",
    "print(\"The total number of tokens is:\", tokens)\n",
    "characters = count_characters(text)\n",
    "print(\"The total number of characters is:\", characters)\n",
    "vocab = count_distinct_words(text)\n",
    "print(\"The total number of distinct words is:\", vocab)\n",
    "topn = count_most_N_frequent(text)\n",
    "print(\"The total number of tokens corresponding to the top-N most frequent words in the vocabulary is:\", topn)\n",
    "ratio = token_type_ratio(text)\n",
    "print(\"The token/type ratio in the dataset is:\", ratio)\n",
    "dev_not_train = dev_vs_train(dev_dataset, text)\n",
    "print(\"The number of types that appear in the dev data but not the training data is:\", dev_not_train)\n",
    "(avg, deviation) = avg_and_standart_deviation(text, characters_dataset)\n",
    "print(\"The average number and standard deviation of characters per token are: \", \"avg:\", avg, \"deviation:\", deviation)\n",
    "\n",
    "\n",
    "for i in range(2, 5):\n",
    "    n_grams = diff_ngrams(text, i)\n",
    "    print(\"The total number of distinct n-grams (of words) that appear in the dataset for n =\",i, n_grams)\n",
    "\n",
    "for i in range(2, 8):\n",
    "    n_grams = diff_ngrams(characters_dataset, i)\n",
    "    print(\"The total number of distint n-grams of characters that appear for n =\",i, n_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tokens is 978726, which is pretty close to our estimation and thus is not very surprising.\n",
    "\n",
    "The number of characters is 5,101,618 which is longer in 100,000 than our estimation, meaning the text contained longer words than we expected.\n",
    "\n",
    "The number of distinct words is 9996 which is twice than what we expected and thus a little bit surprising.\n",
    "\n",
    "The number of tokens corresponding to the top-N most frequent words is 9998 which is very close to the number we assumed.\n",
    "\n",
    "The number of token/type is 97.91176470588235 which is about half than what we thought.\n",
    "\n",
    "The number of types that appear in the dev data but not the training data is 0 which is not much less than we thought.\n",
    "\n",
    "The average number and standard deviation of characters per token is (avg: 5.08356169142334 deviation: 2.5113567920948805) which is pretty close to out estimation.\n",
    "\n",
    "The total number of distinct n-grams (of words) that appear in the dataset for n=2,3,4 are indeed 978725, 978724, 978723\n",
    "\n",
    "As we can see the numbers of distinct n-grams (of characters) that appear in the dataset for n=2,3,4,5,6,7 are indeed 4975413, 4975412, 4975411, 4975410, 4975409, 4975408.\n",
    "\n",
    "Now one last thing is to check whether the Penn Treebank dataset follows the power law distribution. We will check this with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "\n",
    "text = open(\"simple-examples/data/ptb.train.txt\").read()\n",
    "\n",
    "plt.loglog([val for word,val in Counter(text).most_common(4000)])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a relative change in frequency results in the proportional relative change in the rank, hence it follows the power low distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 n-gram Word Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import word_tokenize, ConditionalFreqDist, ConditionalProbDist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_word_lm(data, n=2):\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~ \" * (n-1)\n",
    "    data = pad + data\n",
    "    data = data.split()\n",
    "\n",
    "    for i in range(len(data)-n+1):\n",
    "        history, word = data[i:i+n-1], data[i+n-1]\n",
    "        history = \" \".join(history)\n",
    "        lm[history][word]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]\n",
    "    outlm = {hist:normalize(words) for hist, words in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure used in the model is a dictionary where the keys are all the sequences of n words from the text and the values are counter objects used for the amount of times every word appeared after every sequence. The dictionary is of size at most the number of N-grams multiplied by the size of the vocabulary - so we get:\n",
    "O((|tokens| - n)* |vocabulary|)\n",
    "\n",
    "\n",
    "Let's test the function on a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"the book is green the book is blue the book is green\"\n",
    "\n",
    "lm = train_word_lm(data)\n",
    "print(lm[\"the\"])\n",
    "\n",
    "lm = train_word_lm(data, n=3)\n",
    "print(lm[\"book is\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LangModelI:\n",
    "    lm = \"\"\n",
    "    probDist = \"\"\n",
    "\n",
    "    def __init__(self,text, n=2):\n",
    "        self.lm = train_word_lm(text, n)\n",
    "\n",
    "    def get_most_likely_word(self, seq):\n",
    "        return max(dict(self.lm[seq]), key=dict(self.lm[seq]).get)\n",
    "\n",
    "    def get_prob(self,seq, word,):\n",
    "        p = dict(self.lm[seq]).get(word)\n",
    "        if p is None:\n",
    "            return 0;\n",
    "        return p\n",
    " \n",
    "\n",
    "\n",
    "data = \"the book is green the book is blue the book is green\"\n",
    "\n",
    "m1 = LangModelI(data)\n",
    "m2 = LangModelI(data,3)\n",
    "\n",
    "#using the model to get the probability of a word given an history sequence\n",
    "print(\"The probability for \\\"is\\\" to appear after \\\"book\\\" is: \")\n",
    "print(m1.get_prob(\"book\", \"is\"), \"\\n\")\n",
    "\n",
    "\n",
    "#using the model to get the probability of a word given an history sequance\n",
    "print(\"The probability for \\\"green\\\" to appear after \\\"book is\\\" is: \")\n",
    "print(m2.get_prob(\"book is\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll update the class to include functions that will help us compute the preplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModelI:\n",
    "    lm = ''\n",
    "    prob_dist = ''\n",
    "    n = ''\n",
    "    estimator = ''\n",
    "    fileName = ''\n",
    "    text = ''\n",
    "\n",
    "    def __init__(self, fileName, n=2, estimator=None):\n",
    "        self.fileName = fileName\n",
    "        self.n = n\n",
    "        self.init_text()\n",
    "        self.lm = train_word_lm(self.text, n)\n",
    "        self.estimator = estimator\n",
    "        if self.estimator is not None:\n",
    "            self.init_prob_factory()\n",
    "\n",
    "\n",
    "    def init_text(self):\n",
    "        f = open(self.fileName, \"r\")\n",
    "        self.text = f.read()\n",
    "\n",
    "    def get_most_likely_word(self, seq):\n",
    "        return max(dict(self.lm[seq]), key=dict(self.lm[seq]).get)\n",
    "\n",
    "    def get_prob(self, seq, word):\n",
    "        if self.estimator is None:\n",
    "            p = dict(self.lm[\" \".join(seq)]).get(word)\n",
    "        else:\n",
    "            p = self.prob_dist[seq].prob(word)\n",
    "        if p is None:\n",
    "            return 0;\n",
    "        return p\n",
    "\n",
    "    def get_logprob(self, seq, word):\n",
    "        if self.estimator is not None:\n",
    "            return - self.prob_dist[seq].logprob(word)\n",
    "        else:\n",
    "            p = dict(self.lm[seq]).get(word)\n",
    "            if p is None:\n",
    "                return 0\n",
    "            return p\n",
    "\n",
    "\n",
    "    def init_prob_factory(self):\n",
    "        split_text = (self.text).split()\n",
    "        cfd = nltk.ConditionalFreqDist(\n",
    "            (\" \".join(split_text[i: i + self.n - 1]), \"\".join(split_text[i + self.n - 1]))\n",
    "            for i in range(len(split_text) - self.n + 1))\n",
    "\n",
    "        self.prob_dist = nltk.ConditionalProbDist(cfd, self.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_word_and_context_pairs returns the ngrams organized as pairs of (context, word).\n",
    "\n",
    "'perplexity' function compute the perplexity of the model by using the function 'get_entropy' and computing 2 ^ entorpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_and_context_pairs(text,n):\n",
    "    pairs = []\n",
    "    for i in range(n - 1, len(text)):\n",
    "        context, word = tuple(text[i - n + 1:i]), text[i]\n",
    "        context = \" \".join(context)\n",
    "        pairs.append((context,word))\n",
    "    return pairs\n",
    "\n",
    "def get_entropy(lm, text, n=2):\n",
    "    sum = 0\n",
    "    pairs = get_word_and_context_pairs(text,n)\n",
    "    for (context, word) in pairs:\n",
    "        sum += lm.get_logprob(context, word)\n",
    "    return sum / (len(text) - n + 1)\n",
    "\n",
    "def perplexity(lm, text, n=2):\n",
    "    return 2 ** get_entropy(lm,text, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use it to get the perplexity values of the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, 0.01, fd.B() + 100)\n",
    "lm = LangModelI('simple-examples/data/ptb.valid.txt', 2, lidstone_estimator)\n",
    "\n",
    "f = open(\"simple-examples/data/ptb.test.txt\", \"r\")\n",
    "test_text = f.read()\n",
    "test_text = test_text .split()\n",
    "\n",
    "print(perplexity(lm, test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to change the model to use a different estimators the class includes a function named init_prob_factory which creates the ConditionalProbDist object.\n",
    "\n",
    "now we can use it to get the perplexity of the trained model on the validation dataset for a variety of hyper-parameter gamna.\n",
    "The lower the perplexity the better the model so we'll try to find the best gamma by finding the one that give us the lowest perplexity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('simple-examples/data/ptb.test.txt', \"r\")\n",
    "test_text = f.read()\n",
    "test_text = test_text .split()\n",
    "\n",
    "gammas = np.array([])\n",
    "p_vals = np.array([])\n",
    "\n",
    "opt_gamma = 1\n",
    "min_p = 1000\n",
    "\n",
    "for i in np.arange(0.01, 0.99, 0.03):\n",
    "    gammas = np.append(gammas, i)\n",
    "    lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, i, fd.B() + 100)\n",
    "    lm = LangModelI('simple-examples/data/ptb.valid.txt', 2, lidstone_estimator)\n",
    "    p_i = perplexity(lm, test_text)\n",
    "    p_vals = np.append(p_vals,p_i)\n",
    "    if p_i < min_p:\n",
    "        opt_gamma = i\n",
    "        min_p = p_i\n",
    "\n",
    "plt.plot(gammas, p_vals,'b')\n",
    "plt.show()\n",
    "\n",
    "print(\"The best gamma is: \", opt_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we would like to improve the model by using an n-gram model with increasing values of n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('simple-examples/data/ptb.test.txt', \"r\")\n",
    "test_text = f.read()\n",
    "test_text = test_text .split()\n",
    "\n",
    "opt_n = 0;\n",
    "min_p = 1000\n",
    "n_vals = np.array([])\n",
    "p_vals = np.array([])\n",
    "\n",
    "for i in range(2, 21):\n",
    "    n_vals = np.append(n_vals, i)\n",
    "    lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, opt_gamma, fd.B() + 100)\n",
    "    lm = LangModelI('simple-examples/data/ptb.valid.txt', i, lidstone_estimator)\n",
    "    p_i = perplexity(lm, test_text, i)\n",
    "    p_vals= np.append(p_vals,p_i)\n",
    "    if p_i < min_p:\n",
    "        opt_n = i\n",
    "        min_p = p_i\n",
    "\n",
    "\n",
    "plt.plot(n_vals, p_vals,'g')\n",
    "plt.plot(n_vals, p_vals, 'bo')\n",
    "plt.show()\n",
    "\n",
    "print(\"The best n is:\", opt_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the best predicted n-gram model based on a Lidstone model with the optimized gamma parameter and of the best possible n order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidstone_estimator = lambda fd: nltk.LidstoneProbDist(fd, opt_gamma, fd.B() + 100)\n",
    "lm = LangModelI('simple-examples/data/ptb.valid.txt', opt_n, lidstone_estimator)\n",
    "\n",
    "print(perplexity(lm, test_text, opt_n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best preplixty we could find online is 46.6 which is better than the one we got-  87.9751.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Generating Text from a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def create_word(model, history):\n",
    "    r = random()\n",
    "    for word, prob in model[history]:\n",
    "        r = r - prob\n",
    "        if r <= 0 :\n",
    "            return word\n",
    "\n",
    "\n",
    "def generate(model, seed):\n",
    "    history = \" \".join(seed)\n",
    "    out = []\n",
    "    for i in range(0, 100):\n",
    "        c = create_word(model, history)\n",
    "        history += \" \" + c\n",
    "        history = history.split()\n",
    "        history = history[1:]\n",
    "        history = \" \".join(history)\n",
    "        out.append(c)\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on our model, for n=3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_word_lm(\"simple-examples/data/ptb.valid.txt\", 4)\n",
    "print(generate(lm, [\"ban\", \"on\", \"ivory\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating when the seed is shorter that the history length of the n-gram model maybe can be done by adding a random word from the vocabulary to the seed, or basing the implementation regardless of the history. We might want that the genration will stop when the text is shorter than the original text, so if our model is good it will generate a readable text but if it is not, we will stop in time to see what we can fix.\n",
    "Now let's generate segments on 5 different seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(lm, [\"especially\", \"hard\", \"at\"]))\n",
    "print()\n",
    "print(generate(lm, [\"statement\", \"of\", \"the\"]))\n",
    "print()\n",
    "print(generate(lm, [\"specially\", \"made\", \"for\"]))\n",
    "print()\n",
    "print(generate(lm, [\"N\", \"east\", \"germans\"]))\n",
    "print()\n",
    "print(generate(lm, [\"mr.\", \"fromstein\", \"said\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that as we change the seeds we see different results that are based on the words created from the given history as a seed. For example, in seed like the first there is a discussion about certain task, which makes sense in relation to something that is \"especially hard at\".\n",
    "The last text talks about a person \"he\" which is reasonable given the seed \"mr. Fromstein said\" which \"mr. Fromstein\" is a proper name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A temperature argument can control the level of variability generated by the model by that it increases the probability of each word and thus changes it, so the word is not necessarily the most likely one to be chosen and some words that are less common in a given context can be chosen over the others.\n",
    "The code in the method generator.py from Sameer Sing corresponds to the mathematical explanation provided in the blog by that it calculates the probability of the word, then divides it by the temperature, then calculates the sum of log2 of (2 ^ the quotient + 2 ^ the previous quotient), then for each result again calculates the sum of log2 of (2 ^ each result + 2 ^ the previous result), then raises 2 by the power of the differences between the two sums and if it greater then the random number then the word is being chosen. So the temperature increases each probability and then affects the next word that will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Character language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy Summary:\n",
    "\n",
    "In the article we can observe that RNN language models are able to generate different texts using an input related to a certain field. For example, receiving different inputs of code samples, it can create code blocks that probably won’t compile, but will be perfectly indented and will have balanced brackets, meaning it supplies a very good frame to a code and in addition a code that is very close to a code that works. The thing that is most surprising in the experimental results reported in the blog is that while the common belief at the time the results were published was that RNNs were difficult to train, it is in fact pretty easy and powerful. We witness that by the results of the experiments shown in the article, such as the code block we discussed earlier, a Wikipedia text that the model generated pretty close to the origin, Shakespeare monologues that even though we can see it’s not a Shakespeare’s real text, the model still created pretty good monologues in an eloquent English. Another interesting result is of creating a Latex sampled algebraic geometry. It teaches us that the model is quite good at learning complicated syntactic structures. In conclusion, RNNs are good models to use for training.\n",
    "\n",
    "The unreasonable effectiveness of Characters-level Language Model By Yoav Goldberg Summary:\n",
    "\n",
    "This article discusses the power of language models that do not use smoothing (unlike RNNs models that do use smoothing). The main claim of the writer is that unsmoothed maximum likelihood characters level language models have effectiveness on generating convincing natural language outputs. The unsmoothed maximum likelihood characters level language model works as follows. It calculates the probability of each character to appear next to a history of n characters according to the total number of times the history characters appear in the text in the same order. The article presents a few examples to how the using of that kind of model generates different English texts like a text based on Shakespeare’s sample. It shows that as greater the n as better the text the model generates. The writer is mostly impressive by the context awareness of the model. He says that the model succeeded in learning the context by the history given, for example learned well how to create nested brackets and indented blocks within a code. Finally, the writer tries the model on Linux-kernel code. On these results, he is not very satisfied by the results of the maximum likelihood characters level language models and thus concludes that LSTM, one of RNNs that uses smoothed language models, is doing its job well on texts like this, and thus summarizes that he is impressed by RNNs after all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will gather the recipes dataset and prepare a dataset reader according to the structure of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_preprocess([\"neural_net_cooking_recipes.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic statistics about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "dataset = open(\"neural_net_cooking_recipes.txt.out\").read()\n",
    "tokenized = dataset.split()\n",
    "\n",
    "recipes = 0\n",
    "for i in range(0, (len(tokenized) - 1)):\n",
    "    if tokenized[i] == \"recipe\" and tokenized[i + 1] == \"via\":\n",
    "        recipes += 1\n",
    "print(\"The number of recipes in the dataset is: \", recipes)\n",
    "\n",
    "tokens = count_tokens(dataset)\n",
    "print(\"The number of tokens in the dataset is: \", tokens)\n",
    "\n",
    "characters = count_characters(dataset)\n",
    "print(\"The number of characters in the dataset is: \", characters)\n",
    "\n",
    "vocab = count_distinct_words(dataset)\n",
    "print(\"The size of the vocabulary of the dataset is: \", vocab)\n",
    "\n",
    "def count_dist(word_or_char):\n",
    "    recipes_sizes = []\n",
    "    curr_size = 0\n",
    "    for i in range(0, len(tokenized)):\n",
    "\n",
    "        if tokenized[i] != \"v805\":\n",
    "            if word_or_char == \"word\":\n",
    "                curr_size += 1\n",
    "            elif word_or_char == \"char\":\n",
    "                curr_size += len(tokenized[i])\n",
    "            continue\n",
    "        else:\n",
    "            recipes_sizes.append(curr_size)\n",
    "            curr_size = 0\n",
    "    recipes_sizes.append(curr_size)\n",
    "\n",
    "    recipes_sizes = recipes_sizes[1:len(recipes_sizes)]\n",
    "    return recipes_sizes\n",
    "    \n",
    "recipes_sizes = count_dist(\"word\")\n",
    "pyplot.plot(recipes_sizes)\n",
    "print(\"The distribution of the size of recipes in words in the dataset is:\")\n",
    "pyplot.show()\n",
    "\n",
    "dist_recipes_chars = count_dist(\"char\")\n",
    "pyplot.plot(dist_recipes_chars)\n",
    "print(\"The distribution of the size of recipes in chars in the dataset is: \")\n",
    "pyplot.show()\n",
    "\n",
    "dist_len_words = []\n",
    "for word in tokenized:\n",
    "    dist_len_words.append(len(word))\n",
    "\n",
    "pyplot.plot(dist_len_words)\n",
    "print(\"The distribution of length of words in the dataset is:\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the dataset into training, dev and test as a 80%/10%/10% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"recipes.train.txt\", \"w+\")\n",
    "dev_file = open(\"recipes.dev.txt\", \"w+\")\n",
    "test_file = open(\"recipes.test.txt\", \"w+\")\n",
    "\n",
    "lines = dataset.splitlines()\n",
    "eighty = int(0.8 * len(lines))\n",
    "ten = int(0.1 * len(lines))\n",
    "training_words = []\n",
    "\n",
    "\n",
    "for i in range(0, eighty):\n",
    "    training_words.append(lines[i])\n",
    "training_words = \"\\n\".join(training_words)\n",
    "train_file.write(training_words)\n",
    "\n",
    "dev_words = []\n",
    "for i in range(eighty, eighty + ten):\n",
    "    dev_words.append(lines[i])\n",
    "dev_words = \"\\n\".join(dev_words)\n",
    "dev_file.write(dev_words)\n",
    "\n",
    "\n",
    "test_words = []\n",
    "for i in range(eighty + ten, len(lines)):\n",
    "    test_words.append(lines[i])\n",
    "test_words = \"\\n\".join(test_words)\n",
    "test_file.write(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the order of the char n-gram according to the indications given in Yoav Goldberg's article we will choose n = 3. In Yoav Goldberg's article we've seen that n = 10 worked pretty well on a very long text. \n",
    "Here we have a much shorter text, hence we expect for good results for a smaller n.\n",
    "We will train a char language model using Yoav Goldberg's code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------taken from Yoav Goldberg's assay The unreasonable effectiveness of Character-level Language Models (It stated in the assignment that we may use it!) --------\n",
    "\n",
    "from collections import *\n",
    "\n",
    "def train_char_lm(fname, order):\n",
    "    data = open(fname).read()\n",
    "    lm = defaultdict(Counter)\n",
    "    pad = \"~\" * order\n",
    "    data = pad + data\n",
    "    for i in range(len(data)-order):\n",
    "        history, char = data[i:i+order], data[i+order]\n",
    "        lm[history][char]+=1\n",
    "    def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm\n",
    "\n",
    "from random import random\n",
    "\n",
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        x = random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c\n",
    "\n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "    return \"\".join(out)\n",
    "\n",
    "# --------end of taken from Yoav Goldberg's assay The unreasonable effectiveness of Character-level Language Models--------\n",
    "\n",
    "lm = train_char_lm(\"recipes.dev.txt\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "\n",
    "my_text = generate_text(lm, 3)\n",
    "validation_sentences = [my_text]\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                    for sent in validation_sentences]\n",
    "\n",
    "\n",
    "n = 3\n",
    "model = MLE(3)\n",
    "validation_data, padded_vocab = padded_everygram_pipeline(n, tokenized_text)\n",
    "model.fit(validation_data, padded_vocab)\n",
    "\n",
    "validation_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "\n",
    "\n",
    "for i, valid in enumerate(validation_data):\n",
    "    print(\"The perplexity is:\", model.perplexity(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample about 5 generated recipes from the trained language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    my_text = generate_text(lm, 3)\n",
    "\n",
    "    print(\"sample\", i, \":\")\n",
    "    print()\n",
    "    print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observations:\n",
    "\n",
    "1) We can see that the model uses commands like \"cool\", \"stir\" and \"boil\" in the beginning of the sentences, as it should.\n",
    "\n",
    "2) We also see that the model uses conjunctions like \"to\", \"and\", \"or\", etc. in the right place (For example, \"store olive of cooks\").\n",
    "\n",
    "3) Sometimes the model repeats words, for example \"the the\".\n",
    "\n",
    "4) The model learned some foods names and used them in its recipes (like \"oil\", \"pepper\", etc.).\n",
    "\n",
    "5) The model still created some odd words like \"servingree\", i.e it merged two words that it wasn't supposed to merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Polynomial Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(N, f, sigma):\n",
    "    mu = 0.0\n",
    "\n",
    "    x = np.linspace(0,1,N)\n",
    "\n",
    "    t = np.array([])\n",
    "    for i in range(0, N):\n",
    "        t = np.append(t, f(x[i]) + rn.normal(mu, sigma))\n",
    "\n",
    "    return (x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we'll draw a plot of (x,t) using matplotlib for N=100 and the function sin(2πx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def y(x):\n",
    "    return math.sin(2 * math.pi * x)\n",
    "\n",
    "(x, t) = generateDataset(100, y, 0.03)\n",
    "plt.plot(x, t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Polynomial Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given (x,t) we will attempt to estimate a vector w of size M that will minimize the square error function. \n",
    "\n",
    "let's add a function to compute the design matrix according to the defntion:\n",
    "matrix Φ such that Φnm = xn^m = Φm(xn).\n",
    "\n",
    "we'll than use the design matrix to compute WLS by the rule:\n",
    "WLS = (Φ^TΦ)-1ΦTt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_matrix(x,M):\n",
    "    design_matrix = np.ones(x.size)\n",
    "    for i in range(1,M+1):\n",
    "        column_i = [(curr_x**i) for curr_x in x]\n",
    "        design_matrix = np.column_stack([design_matrix,column_i])\n",
    "\n",
    "    return design_matrix\n",
    "\n",
    "\n",
    "def OptimizeLS(x, t, M):\n",
    "    phi = get_design_matrix(x,M)\n",
    "    prod = np.dot(phi.T, phi)\n",
    "    i = np.linalg.inv(prod)\n",
    "    m = np.dot(i, phi.T)\n",
    "    w = np.dot(m, t)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's add a simple function that will help us later,\n",
    "\n",
    "poly get x and a list of coefficients and return the value of the polynomial defined by the coefficients for x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(coeff,x):\n",
    "    fx = 0\n",
    "    for i in range(0,coeff.size):\n",
    "        fx = fx +coeff[i]*(x**i)\n",
    "    return fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can demonstrate the result for different parameters.\n",
    "we'll use the function sin(2πx), dataset of size N=10 and M=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the real function curve\n",
    "x_real = np.arange(0, 1, 0.01)\n",
    "t_real = [y(x) for x in x_real]\n",
    "plt.plot(x_real, t_real,'g')\n",
    "\n",
    "(x, t) = generateDataset(10, y, 0.03)\n",
    "plt.plot(x, t,'ro')\n",
    "\n",
    "w = OptimizeLS(x, t, 1)\n",
    "t_new = [poly(w,xi) for xi in x]\n",
    "\n",
    "plt.plot(x, t_new,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we can tell the Least Squares method preforemed pretty badly. let's increase change M from 1 to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_real, t_real,'g')\n",
    "\n",
    "(x, t) = generateDataset(10, y, 0.03)\n",
    "plt.plot(x, t,'ro')\n",
    "\n",
    "w = OptimizeLS(x, t, 3)\n",
    "t_new = [poly(w,xi) for xi in x]\n",
    "\n",
    "plt.plot(x, t_new,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 5\n",
    "plt.plot(x_real, t_real,'g')\n",
    "\n",
    "(x, t) = generateDataset(10, y, 0.03)\n",
    "plt.plot(x, t,'ro')\n",
    "\n",
    "w = OptimizeLS(x, t,5)\n",
    "t_new = [poly(w,xi) for xi in x]\n",
    "\n",
    "plt.plot(x, t_new,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = 10\n",
    "plt.plot(x_real, t_real,'g')\n",
    "\n",
    "(x, t) = generateDataset(10, y, 0.03)\n",
    "plt.plot(x, t,'ro')\n",
    "\n",
    "w = OptimizeLS(x, t, 10)\n",
    "t_new = [poly(w,xi) for xi in x]\n",
    "\n",
    "plt.plot(x, t_new,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the method got the best result for M = 5. For greater values it fitts some of the data points but the fitted curve oscillates outside of them and gives a poor representation of the function sin(2πx).  This behaviour is known as over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Polynomial Curve Fitting with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid over-fitting, we will use a method called regularization.\n",
    "\n",
    "optimizePLS returns the optimal parameters W_PLS given M and a hyper-parameter lambda. \n",
    "generateDataset3(N, f, sigma) create 3N equi-distant values and divides them to 3 randomly shuffled sets of size N-\n",
    "train, validate and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizePLS(x, t, M, lambdaVar):\n",
    "    # wPLS = (ΦTΦ + λI)-1ΦTt\n",
    "    phi = get_design_matrix(x, M)\n",
    "    lambda_id = lambdaVar*np.identity(M+1)\n",
    "    prod = np.dot(phi.T, phi) + lambda_id\n",
    "    i = np.linalg.inv(prod)\n",
    "    m = np.dot(i, phi.T)\n",
    "    w = np.dot(m, t)\n",
    "\n",
    "    return w\n",
    "\n",
    "def generateDataset3(N, f, sigma):\n",
    "    mu = 0.0\n",
    "    x = np.linspace(0, 1, 3*N)\n",
    "\n",
    "    #divide the set into 3 subsets of size N\n",
    "    x_train = x[:(N)]\n",
    "    x_valid = x[N:N * 2]\n",
    "    x_test = x[N * 2:N * 3]\n",
    "\n",
    "    #shuffle the sets\n",
    "    np.random.shuffle(x_train)\n",
    "    np.random.shuffle(x_valid)\n",
    "    np.random.shuffle(x_test)\n",
    "\n",
    "    #compute the t's sets\n",
    "    t_train =  np.array([])\n",
    "    t_valid =  np.array([])\n",
    "    t_test = np.array([])\n",
    "    for i in range(0, N):\n",
    "        t_train = np.append(t_train, f(x_train[i]) + rn.normal(mu, sigma))\n",
    "        t_valid = np.append(t_valid, f(x_valid[i]) + rn.normal(mu, sigma))\n",
    "        t_test = np.append(t_test, f(x_test[i]) + rn.normal(mu, sigma))\n",
    "\n",
    "    return (x_train,t_train),(x_valid,t_valid),(x_test,t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalized_error will compute the normalized error for a given W = W0..Wm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_error(x,t,w,N):\n",
    "    err = 0\n",
    "    for i in range(N):\n",
    "        poly_i = 0\n",
    "        for m in range(w.size):\n",
    "            poly_i = poly_i + w[m]*(x[i]**m)\n",
    "        err = err + (t[i] - poly_i)**2\n",
    "    err = err**0.5\n",
    "    err = err*(1/N)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizePLS2 will use the training set to compute the W's and use the validate set to choose the optimal lambda.\n",
    "it will also return an 'error vector' which holds the error for every lamdba.\n",
    "\n",
    "The optimal lambda will be selected from the interval (2^-40, 2^-20) and will be the one the has the smallest normalized error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizePLS2(xt, tt, xv, tv, M):\n",
    "    lambda_vals = np.array([])\n",
    "    lambda_err = np.array([])\n",
    "    min_err = 1000  #initialized to a max value \n",
    "    opt_lambda = 0\n",
    "    \n",
    "    for i in range(-40,-19):\n",
    "        \n",
    "        #add the new lambda value to needs to be checked\n",
    "        curr_lambda = 2**i\n",
    "        lambda_vals = np.append(lambda_vals, curr_lambda)\n",
    "        \n",
    "        #find W0...Wm by using the training set:\n",
    "        w =optimizePLS(xt,tt,M,curr_lambda) \n",
    "        \n",
    "        #now we want to save the error for this lambda using the vaildate set\n",
    "        err = normalized_error(xv, tv, w, xv.size)\n",
    "        lambda_err= np.append(lambda_err, err)\n",
    "        \n",
    "        # save it if it's the current minimal error\n",
    "        if(err < min_err):\n",
    "            min_err = err\n",
    "            opt_lambda = curr_lambda\n",
    "\n",
    "    return (lambda_vals,lambda_err,opt_lambda,min_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can create the 3 sets for N=10 and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,t_train),(x_valid,t_valid),(x_test,t_test) = generateDataset3(10, y, 0.03)\n",
    "\n",
    "(lambda_vals,lambda_err,opt_lambda, min_err) = optimizePLS2(x_train, t_train, x_valid, t_valid, 5)\n",
    "\n",
    "\n",
    "\n",
    "#sort the array so that plot will print it nicely...\n",
    "x_test, t_test = zip(*sorted(zip(x_test, t_test)))\n",
    "x_test = np.asarray(x_test)\n",
    "t_test = np.asarray(t_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's draw the plot of the normalized error of the model for N=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambda_vals,lambda_err,'r')\n",
    "plt.plot(opt_lambda, min_err, 'g*')\n",
    "plt.show()\n",
    "\n",
    "print(\"the optimal lambda is: \", opt_lambda)\n",
    "print(\"it's normalized_error is: \", min_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the minimum point in the graph (marked with green) is of the optimal lambda and it's normalized error.\n",
    "\n",
    "now we'll use the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = optimizePLS(x_test, t_test, 5, opt_lambda)\n",
    "t_new = [poly(w,xi) for xi in x_test]\n",
    "\n",
    "plt.plot(x_test, t_test,'g')\n",
    "\n",
    "plt.plot(x_test, t_new,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's repeat the process for N=100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,t_train),(x_valid,t_valid),(x_test,t_test) = generateDataset3(100, y, 0.03)\n",
    "\n",
    "(lambda_vals,lambda_err,opt_lambda, min_err) = optimizePLS2(x_train, t_train, x_valid, t_valid, 5)\n",
    "\n",
    "\n",
    "#sort the array so that plot will print it nicely...\n",
    "x_test, t_test = zip(*sorted(zip(x_test, t_test)))\n",
    "x_test = np.asarray(x_test)\n",
    "t_test = np.asarray(t_test)\n",
    "\n",
    "plt.plot(lambda_vals,lambda_err,'r')\n",
    "plt.plot(opt_lambda, min_err, 'g*')\n",
    "plt.show()\n",
    "\n",
    "print(\"the optimal lambda is: \", opt_lambda)\n",
    "print(\"it's normalized_error is: \", min_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set \n",
    "w = optimizePLS(x_test, t_test, 5, opt_lambda)\n",
    "t_new = [poly(w,xi) for xi in x_test]\n",
    "\n",
    "plt.plot(x_test, t_test,'g')\n",
    "\n",
    "plt.plot(x_test, t_new,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the normalized error is smaller for N=100 comapre to N=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Probabilistic Regression Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import math\n",
    "from numpy import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def phi(x):\n",
    "    return np.array([x ** i for i in range(M + 1)]).reshape((M + 1, 1))\n",
    "\n",
    "def get_S(x, M, alpha, sigma2):\n",
    "    I = np.identity(M+1)\n",
    "    sum = np.zeros((M+1, M+1))\n",
    "    for n in range(len(x)):\n",
    "        sum += np.dot(phi(x[n]), phi(x[n]).T)\n",
    "    sum = sum*(1/sigma2)\n",
    "    S_inv = alpha*I + sum\n",
    "    S = np.linalg.inv(S_inv)\n",
    "    return S\n",
    "\n",
    "def bayesianEstimator(x_train, t_train, M, alpha, sigma2):\n",
    "    S = get_S(x_train, M, alpha, sigma2)\n",
    "    return (lambda x: get_m(x, x_train, t_train, M, alpha, sigma2, S),\n",
    "            lambda x: get_var(x,sigma2, S))\n",
    "\n",
    "\n",
    "# get the Mean of predictive distribution\n",
    "def get_m(x, x_train, t_train, M, alpha, sigma2, S):\n",
    "    sum = np.array(zeros((M+1, 1)))\n",
    "    for n in range(len(x_train)):\n",
    "        sum += np.dot(phi(x_train[n]), t_train[n])\n",
    "    return (1/sigma2) * phi(x).T.dot(S).dot(sum)\n",
    "\n",
    "\n",
    "# get the Variance of predictive distribution\n",
    "def get_var(x,sigma2, S):\n",
    "    return sigma2 + phi(x).T.dot(S).dot(phi(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the mean and variance of the predictive distribution inferred from the dataset.\n",
    "\n",
    "First let's use N=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "sigma2 = 1/11.1\n",
    "M = 9\n",
    "\n",
    "def y(x):\n",
    "    return math.sin(2 * math.pi * x)\n",
    "\n",
    "# get the real function curve\n",
    "x_real = np.arange(0, 1, 0.01)\n",
    "t_real = [y(x) for x in x_real]\n",
    "\n",
    "(x_train, t_train) = generateDataset(10, y, 0.03)\n",
    "\n",
    "m, var = bayesianEstimator(x_train, t_train, M, alpha, sigma2)\n",
    "\n",
    "mean = [m(x)[0,0] for x in x_real]\n",
    "variance = [var(x)[0, 0] for x in x_real]\n",
    "\n",
    "var_sqrt = np.sqrt(variance)\n",
    "upper = mean + var_sqrt\n",
    "lower = mean - var_sqrt\n",
    "\n",
    "plot(x_train, t_train, 'bo', markerfacecolor='none')\n",
    "plot(x_real, t_real, 'g-')\n",
    "plot(x_real, mean, 'r-')\n",
    "fill_between(x_real, upper, lower, color='pink')\n",
    "xlim(0.0, 1.0)\n",
    "ylim(-1.5, 1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the process for N=100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train) = generateDataset(100, y, 0.03)\n",
    "\n",
    "m, var = bayesianEstimator(x_train, t_train, M, alpha, sigma2)\n",
    "\n",
    "mean = [m(x)[0,0] for x in x_real]\n",
    "variance = [var(x)[0, 0] for x in x_real]\n",
    "\n",
    "var_sqrt = np.sqrt(variance)\n",
    "upper = mean + var_sqrt\n",
    "lower = mean - var_sqrt\n",
    "\n",
    "\n",
    "# plot(x_train, t_train, 'bo', markerfacecolor='none')\n",
    "plot(x_real, t_real, 'g-')\n",
    "plot(x_real, mean, 'r-')\n",
    "fill_between(x_real, upper, lower, color='pink')\n",
    "xlim(0.0, 1.0)\n",
    "ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to make the height of the band around the most likely function very small in one segment of the function and large in another will be to change the way the data points are scattered in the interval 0..1. The more data points in a segment - \n",
    "the smaller the variance and therefore the highet of the band."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Neural Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Summarize the Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task the tutorial addresses is building and training a basic character-level RNN to classify surnames to the right language. A character-level RNN reads words and outputs a prediction and “hidden state” at each step. It then feeds its previous hidden state into each next step. The final prediction is the class the word belongs to.\n",
    "\n",
    "The first step is to prepare the data. we read the files, convert each string from unicode To Ascii and create a dictionary of lists of names per language.\n",
    "\n",
    "The next step is turning names into tensors. To represent a single letter, we use a “one-hot vector” of size |all_letters|. To make a word we join the “one-hot vector” into a 2D matrix.\n",
    "\n",
    "Now we need to create a recurrent neural network using 2 linear layers which operate on an input and hidden state, with a LogSoftmax layer after the output. The next step is to create a training example (a name and its language) and use it to train the network by showing it a bunch of examples, have it make guesses, and tell it if it’s wrong. The loss function being used is NLLLoss -The negative log likelihood loss. Finally, in order to see how well the network performs on different categories, we will create a confusion matrix, indicating for every actual language (rows) which language the network guesses (columns).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Explore City Names Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "'''\n",
    "Don't change these constants for the classification task.\n",
    "You may use different copies for the sentence generation model.\n",
    "'''\n",
    "languages = [\"af\", \"cn\", \"de\", \"fi\", \"fr\", \"in\", \"ir\", \"pk\", \"za\"]\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = languages\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = codecs.open(filename, \"r\",encoding='utf-8', errors='ignore').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# number of categories, tokens per category, number of characters, distinct characters, average number of characters per token\n",
    "\n",
    "datasets = [(\"simple-examples/data/cities/train/af.txt\", \"af\"), (\"simple-examples/data/cities/train/cn.txt\", \"cn\"), (\"simple-examples/data/cities/train/de.txt\", \"de\"), (\"simple-examples/data/cities/train/fi.txt\", \"fi\"), (\"simple-examples/data/cities/train/fr.txt\", \"fr\"),\n",
    "(\"simple-examples/data/cities/train/in.txt\", \"in\"), (\"simple-examples/data/cities/train/ir.txt\", \"ir\"), (\"simple-examples/data/cities/train/pk.txt\", \"pk\"), (\"simple-examples/data/cities/train/za.txt\", \"za\")]\n",
    "print(\"The number of categories is 9 (number of languages)\")\n",
    "for (dataset, name) in datasets:\n",
    "    lines = readLines(dataset)\n",
    "    category_lines[name] = lines\n",
    "    lines_ch = \" \".join(lines).split()\n",
    "    characters = []\n",
    "    for ch in lines_ch:\n",
    "        for i in range(0, len(ch)):\n",
    "            characters.append(ch[i])\n",
    "    lines = set(lines)\n",
    "    print(\"The number of tokens in category\", name, \"is:\", len(lines))\n",
    "    sum_ch = 0\n",
    "    num_of_chars = len(characters)\n",
    "    characters = set(characters)\n",
    "    print(\"The number of characters in category\", name, \"is:\", num_of_chars)\n",
    "    print(\"The number of distinct characters in category\", name, \"is:\", len(characters))\n",
    "    print(\"The average number of characters per token in category\", name, \"is:\", num_of_chars / len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unicodeToAscii is a good idea for this task because it enables to split the lines by \"\\n\", and in this task each line represents a city name so we would like to split it by lines so we will have all the names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train a Model and Evaulate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a one hot vector for each language (We adapted the code of the PyTorch tutorial to our model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass an input - a tensor for the current letter and a previous hidden state, to run a step on this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = lineToTensor('nokchinni')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We received a tensor where every item is the likelihood of the category.\n",
    "Let's interpret the output of the network and get the index of the greatest value of the likelihood of the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some examples of a name and its language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will want that the network will make guesses by input of examples, and we will tell if it's wrong. We will choose nn.NLLLoss function to the loss function because the last layer of the RNN is nn.LogSoftmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003 # We want a value which is not too low and not too high\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function returns both the output and loss. Let's run it with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results and conclude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that as the time passes the guesses are less correct.\n",
    "We will create a confusion matrix and see how well the network performs on the different categories. For every language (rows) we will see which language the network guesses (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the results we obtain that the main confusion cases observed in the confusion matrix are:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
